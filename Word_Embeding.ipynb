{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blQVMyxYNHjs"
   },
   "source": [
    "## Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGdW_jhENGo-",
    "outputId": "cb48f0c6-3b46-44ab-bad8-f05642fa33d0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySqCfIhzLyAf"
   },
   "source": [
    "## Read Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86y-ye1ZLXk8"
   },
   "outputs": [],
   "source": [
    "def read_half_of_text_file(file_path):\n",
    "    sentences = []\n",
    "\n",
    "    # Reading the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Calculating the midpoint of the file\n",
    "    midpoint = len(lines) // 2\n",
    "\n",
    "    # Reading only the first half of the file\n",
    "    for line in lines[:midpoint]:\n",
    "        # Stripping any leading/trailing whitespaces\n",
    "        cleaned_line = line.strip()\n",
    "\n",
    "        # Only add non-empty lines\n",
    "        if cleaned_line:\n",
    "            sentences.append(cleaned_line)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uXzrut3fI_W"
   },
   "outputs": [],
   "source": [
    "def read_all_of_text_file(file_path):\n",
    "    sentences = []\n",
    "\n",
    "    # Reading the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Processing all lines in the file\n",
    "    for line in lines:\n",
    "        # Stripping any leading/trailing whitespaces\n",
    "        cleaned_line = line.strip()\n",
    "\n",
    "        # Only add non-empty lines\n",
    "        if cleaned_line:\n",
    "            sentences.append(cleaned_line)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeS5wV0_L375",
    "outputId": "35bcac15-5a73-4ca2-e092-539d4cb06413"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sentences = read_half_of_text_file('/content/drive/MyDrive/captions_output.txt')\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HH-u6CASR4Z_",
    "outputId": "0096ac7b-5bf0-4006-9e45-7b4dc63026e2"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sentences = read_all_of_text_file('/content/drive/MyDrive/captions_output.txt')\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChZ2laHTNQ2z"
   },
   "source": [
    "## ConcepNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1bfDhJWNVSS"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sys import stdout\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.io import loadmat, savemat\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5kfFzMaNXFD"
   },
   "outputs": [],
   "source": [
    "#ConceptNet NumberBatch\n",
    "\n",
    "def text_to_uri(text):\n",
    "    \"\"\"\n",
    "    An extremely cut-down version of ConceptNet's `standardized_concept_uri`.\n",
    "    Converts a term such as \"apple\" into its ConceptNet URI, \"/c/en/apple\".\n",
    "\n",
    "    Only works for single English words, with no punctuation besides hyphens.\n",
    "    \"\"\"\n",
    "    return '/c/en/' + text.lower().replace('-', '_')\n",
    "\n",
    "def normalize_vec(vec):\n",
    "    \"\"\"\n",
    "    Normalize a vector to a unit vector, so that dot products are cosine\n",
    "    similarities.\n",
    "\n",
    "    If it's the zero vector, leave it as is, so all its cosine similarities\n",
    "    will be zero.\n",
    "    \"\"\"\n",
    "    norm = vec.dot(vec) ** 0.5\n",
    "    if norm == 0:\n",
    "        return vec\n",
    "    return vec / norm\n",
    "\n",
    "class AttributeHeuristic:\n",
    "    def __init__(self, hdf5_filename):\n",
    "        \"\"\"\n",
    "        Load a word embedding matrix that is the 'mat' member of an HDF5 file,\n",
    "        with UTF-8 labels for its rows.\n",
    "\n",
    "        (This is the format that ConceptNet Numberbatch word embeddings use.)\n",
    "        \"\"\"\n",
    "        self.embeddings = pd.read_hdf(hdf5_filename, 'mat', encoding='utf-8')\n",
    "        self.cache = {}\n",
    "\n",
    "    def get_vector(self, term):\n",
    "        \"\"\"\n",
    "        Look up the vector for a term, returning it normalized to a unit vector.\n",
    "        If the term is out-of-vocabulary, return a zero vector.\n",
    "\n",
    "        Because many terms appear repeatedly in the data, cache the result.\n",
    "        \"\"\"\n",
    "        uri = text_to_uri(term)\n",
    "        if uri in self.cache:\n",
    "            return self.cache[uri]\n",
    "        else:\n",
    "            try:\n",
    "                vec = normalize_vec(self.embeddings.loc[uri])\n",
    "            except KeyError:\n",
    "                vec = pd.Series(index=self.embeddings.columns).fillna(0)\n",
    "            self.cache[uri] = vec\n",
    "            return vec\n",
    "\n",
    "    def get_similarity(self, term1, term2):\n",
    "        \"\"\"\n",
    "        Get the cosine similarity between the embeddings of two terms.\n",
    "        \"\"\"\n",
    "        return self.get_vector(term1).dot(self.get_vector(term2))\n",
    "\n",
    "    def compare_attributes(self, term1, term2, attribute):\n",
    "        \"\"\"\n",
    "        Our heuristic for whether an attribute applies more to term1 than\n",
    "        to term2: find the cosine similarity of each term with the\n",
    "        attribute, and take the difference of the square roots of those\n",
    "        similarities.\n",
    "        \"\"\"\n",
    "        match1 = max(0, self.get_similarity(term1, attribute)) ** 0.5\n",
    "        match2 = max(0, self.get_similarity(term2, attribute)) ** 0.5\n",
    "        return match1 - match2\n",
    "\n",
    "    def classify(self, term1, term2, attribute, threshold):\n",
    "        \"\"\"\n",
    "        Convert the attribute heuristic into a yes-or-no decision, by testing\n",
    "        whether the difference is larger than a given threshold.\n",
    "        \"\"\"\n",
    "        return self.compare_attributes(term1, term2, attribute) > threshold\n",
    "\n",
    "    def evaluate(self, semeval_filename, threshold):\n",
    "        \"\"\"\n",
    "        Evaluate the heuristic on a file containing instances of this form:\n",
    "\n",
    "            banjo,harmonica,stations,0\n",
    "            mushroom,onions,stem,1\n",
    "\n",
    "        Return the macro-averaged F1 score. (As in the task, we use macro-\n",
    "        averaged F1 instead of raw accuracy, to avoid being misled by\n",
    "        imbalanced classes.)\n",
    "        \"\"\"\n",
    "        our_answers = []\n",
    "        real_answers = []\n",
    "        for line in open(semeval_filename, encoding='utf-8'):\n",
    "            term1, term2, attribute, strval = line.rstrip().split(',')\n",
    "            discriminative = bool(int(strval))\n",
    "            real_answers.append(discriminative)\n",
    "            our_answers.append(self.classify(term1, term2, attribute, threshold))\n",
    "\n",
    "        return f1_score(real_answers, our_answers, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mz7HmLLgNfxX"
   },
   "outputs": [],
   "source": [
    "heuristic = AttributeHeuristic('/content/drive/MyDrive/numberbatch-20180108-biased.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S5kfDUWNkBQ"
   },
   "source": [
    "## Get Visual Words Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDxpZaiJNnT0"
   },
   "outputs": [],
   "source": [
    "coco_labels = [\n",
    "  'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "  'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
    "  'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "  'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "  'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "  'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "  'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "  'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "  'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n",
    "  'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAq4B6ZSNtUJ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "en_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6WlvfalNuqI"
   },
   "outputs": [],
   "source": [
    "def do_spacy(sentence: str) -> list:\n",
    "    sentence = sentence.lower()\n",
    "    doc = en_nlp(sentence)\n",
    "    visual_words = []\n",
    "    visual_deps = {'amod', 'oprd', 'dobj', 'attr', 'nsubj', 'pobj'}\n",
    "    for token in doc:\n",
    "        if (token.dep_ in visual_deps and token.pos_ in ('NOUN', 'VERB', 'ADJ')) or token.text in coco_labels:\n",
    "            visual_words.append(token.text)\n",
    "    return visual_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_Mqv_0yrXjX"
   },
   "outputs": [],
   "source": [
    "def do_spacy(sentence: str) -> list:\n",
    "    sentence = sentence.lower()\n",
    "    doc = en_nlp(sentence)\n",
    "    visual_words = []\n",
    "    visual_deps = {'amod', 'oprd', 'dobj', 'attr', 'nsubj', 'pobj'}\n",
    "    for token in doc:\n",
    "        if (token.dep_ in visual_deps and token.pos_ in ('NOUN', 'VERB', 'ADJ')) or token.text in coco_labels:\n",
    "            visual_words.append(f\"({token.dep_}, {token.text}, {token.head.text})\")\n",
    "            if token.dep_ == 'dobj':\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == 'amod':\n",
    "                        visual_words.append(f\"({child.dep_}, {child.text}, {token.text})\")\n",
    "                    if child.dep_ == 'det':\n",
    "                        visual_words.append(f\"({child.dep_}, {child.text}, {token.text})\")\n",
    "    return visual_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AdC3lzHTraSH",
    "outputId": "6ec905f9-e467-4f48-b0f0-7d8fadc6a751"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sentence = \"A bearded, bald man wears a multicolored tie\"\n",
    "visual_words = do_spacy(sentence)\n",
    "print(visual_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71_mI8Jfxcqx",
    "outputId": "0ddb4f65-7aaa-498b-ff56-e60384c9dc27"
   },
   "outputs": [],
   "source": [
    "visual_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDp9Wm_1w7Yj",
    "outputId": "1cc9887f-c792-4e66-be9d-d558d7e05f95"
   },
   "outputs": [],
   "source": [
    "for item in visual_words:\n",
    "    # Remove parentheses and split the string by comma\n",
    "    parts = item.strip(\"()\").split(\", \")\n",
    "    if len(parts) == 3:\n",
    "        rel, word1, word2 = parts  # Unpack the parts\n",
    "        print(rel,word1,word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mi5yDKYsNwQC",
    "outputId": "7c644e74-7e0a-4ca5-8f2c-cc4be37acbbd"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sentence = \"A black dog .\"\n",
    "visual_words = do_spacy(sentence)\n",
    "print(visual_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9kD99ouN3Jj"
   },
   "source": [
    "## Save the Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdsnwiclN7OP"
   },
   "outputs": [],
   "source": [
    "mydic={\n",
    "1:['person','doll','girl','poet','character','enemy','individual','people','single','human','baby','stranger','farmer','captain','name','men','doctor','woman','owner','native','friend','judge','man','chief','member','merchant','agent','body','servant','guard','statue','boy','one people','me','neighbour','fellow','being','child','boys'],\n",
    "2:['bicycle','cyclocross','step','hirondelle','biker','cyclocomputer','biciklo','hobby','horse','rower','wielrenner','pogo','monocycle','hitch rack','groupset','bicikal','daawheeyl','rota','exercise bicycle','velosiped','saikul','pneumatic','mount','donk','bicycle touring','velodrome','bidon','bmx'],\n",
    "3:['car','hybrid','four','wheels','wheel','vehicle','wax','accord','transportation', 'seat','passenger', 'motor','ride','automobil','turn','drive','truck','auto','automobile','engine','van','oil','exhaust','brake','automobilski','gas','four wheels','kola','driver','road','driving','motor vehicle','automobilist'],\n",
    "4:['motorcycle','motosikal','low side','smoker','touring motorcycle','dress for slide','scooter' ,'combination','pillion','monkey','chook chaser','moto','motocikl','motocycle','motorcycling','motomondiale','lay down','motorrad','gluaisrothar','ducati','sissy bar','bicycle'],\n",
    "5:['airplane','rapid unplanned' ,'disassembly','aerobatics', 'jet', 'paper airplane','aeroplanum','ditch','biplane','repl','pako','wing','letoun','lidmana','messerschmitt','kiebitz','lentz','aeradio','vliegtoestel','heavier than air','avion','avionski','clipper','abyon','kapal terbang','aeroplan','monoplane','737','toestel'],\n",
    "6:['bus','autobus','buss','knifeboard','bus station','transport','kolodvor','passenger','corriera','remains','headway','zais','component','seka auto','balai','busless','autobusov','kneeling bus','nysse','bondi','sabirnica','ambulance','sakayan','ekspressbuss','omnibus','double-decker','jitney','bus_topology','coach','motorcoach','charabanc','autobus','motorbus','passenger vehicle','busbar','heap','jalopy'],\n",
    "7:['train','school', 'civilise','rail','gear','railroad_train','prepare','take aim','educate','wagon_train','string','take','caravan','trail','check','train','direct','cultivate','geartrain','power train','aim','civilize','coach','gearing','discipline','condition','groom','develop'],\n",
    "8:['truck','sleeper','cab','lorry','chidtsoh','truckyard','wheel','vehicle','knuckle under','run out','kamion','camin','caro','roach coach','truck' ' garden', ' van', ' trucking', ' artic', ' platform' ' crane', ' camionero', ' cart', ' cisterna', ' halyard', ' truck driver','gun carriage','rustbucket','retarder','weigh station','camioncillo','truckless','cami','kamionski','defraud'],\n",
    "9:['boat','sailing','vessel','sail','sails','floating vessel','yacht','water vehicle','vehicle','water','transport','transportation','small ship','passenger','barica','gondola','sailing vessel','ship','craft','water vessel','ocean','floating','water craft','rudder','captain','ark','watercraft','rib','small','sea','barka','canoe','ferry','amac'],\n",
    "10:['traffic light'],\n",
    "11:['fire hydrant'],\n",
    "13:['stop sign'],\n",
    "14:['parking meter'],\n",
    "15:['bench','terrace','judiciary','Bench','workbench','work_bench'],\n",
    "16:['bird','chicken','duck','wings','animal','winged','feathers','flying animal','nest','dove','egg','flying','quail','creature','feathered','chick','canary','bat','pigeon','owl'],\n",
    "17:['cat',' meow','kitty','flea','feline','kitten','maca','cats','mexican hairless',' house',' pet'],\n",
    "18:['dog','woof','tail','cat','flea',' wolf','bark','canine','friend','pet'],\n",
    "19:['horse','ogat','sivac','hackney',' krkalo','race','equine','legs riding','jockey','saddle four legs','centaur','cow','mammal','hooves','racing','tail','hevonen','erav','pony','large','racing animal','struna','mane'],\n",
    "20:['sheep','ram','shepherd','baa','lamb','wool animal','brabonjak','ewe','goat','wooly animal','woolly','baa baa','mutton','wooly','adult lamb','flock','cotton','lambs'],\n",
    "21:['cow','ox','calf','beef','bovine animal','bovine','cattle','moo', 'moo animal'],\n",
    "22:['elephant','fil','elevant','slonovski','elephants'],\n",
    "23:['bear','grizzly','forest','large','brown'],\n",
    "24:['zebra','zeedonk','zbre','zeal','stripe','zebras'],\n",
    "25:['giraffe','camelopard','giraffe','Giraffa camelopardalis'],\n",
    "27:['backpack','back_pack','knapsack','rucksack','backpack','packsack','pack','haversack'],\n",
    "28:['umbrella','provide','brelly','sateenvarjo','ymbarl','purposes pandong','paraple'],\n",
    "31:['handbag','bag','purse','pocketbook'],\n",
    "32:['tie','connect','railroad tie','standoff','link_up', ' attach', ' sleeper', ' tie-in'],\n",
    "33:['suitcase','travelling_bag','suitcase','grip','baggage','luggage'],\n",
    "34:['frisbee','throw','pie','boomerang','turf','disk','record','glide'],\n",
    "35:['skis','slope'],\n",
    "36:['snowboard','droneboarding','lautailla','snowboarder','shredder','snowboards','ski'],\n",
    "37:['sports ball'],\n",
    "38:['kite','barcud','sail','frigi'],\n",
    "39:['baseball bat'],\n",
    "40:['baseball glove'],\n",
    "41:['skateboard','freeboard','kingpin','hoverboard'],\n",
    "42:['surfboard','skurf','longboard','surf','surf riding','boogieboard'],\n",
    "43:['tennis racket'],\n",
    "44:['bottle','bottleful','bottle','nursing bottle','feeding_bottle'],\n",
    "46:['wine glass'],\n",
    "47:['cup','drink','kup','soutien gorge','saucer','mug','pokal','glass','cups','kupa'],\n",
    "48:['fork','ramification','ramify','branching','branch','separate','furcate','pitchfork','forking','crotch'],\n",
    "49:['knife','edge','stick frog','tongue stab'],\n",
    "50:['spoon','spoonful'],\n",
    "51:['bowl','trough', 'arena','sports_stadium','stadium','bowling_ball','bowlful','pipe_bowl','roll'],\n",
    "52:['banana','kentang', 'plantain', 'bann', 'banaaniplantaasi', 'mkungu', 'slip', 'saba'],\n",
    "53:['apple','orchard apple tree', 'Malus pumila'],\n",
    "54:['sandwich','ville', 'sendvi', 'kent'],\n",
    "55:['orange','orangish','orangeness'],\n",
    "56:['broccoli','parsakaali','raphanin','brokula','vegetable', 'brassicaceae','broccolilike' ,'calabrese'],\n",
    "57:['carrot','cenoura','wortel'],\n",
    "58:['hotdog'],\n",
    "59:['pizza','pizzero','slice','pica','pizza margherita','pizzetta','puff','pizzaiola'],\n",
    "60:['donut','sinker','doughnut','munk'],\n",
    "61:['cake','round','sweet','baked','bakery',' birthday candles','desert', 'pie','birthday pastry'],\n",
    "62:['chair','sitting','bedchair','chaire','sit', ' sitting device','seat'],\n",
    "63:['couch','redact','sofa','frame','couch','lounge','cast','put'],\n",
    "64:['potted plant'],\n",
    "65:['bed','sleep in','room','bedroom','blankets'],\n",
    "67:['dining table'],\n",
    "70:['toilet','soil pipe','tiwalet','wc', ' washroom'],\n",
    "72:['tv','tlvision','nightline','rebroadcaster','transvestite','news','tlviseur'],\n",
    "73:['laptop','desktop','computer','fartlva','portatile','skootrekenaar','porttil','snpur','compound','desktop computer','notebook'],\n",
    "74:['mouse','shiner', 'mouse','pussyfoot','computer mouse','sneak','creep','black_eye'],\n",
    "75:['remote','remote_control','remote','distant','removed'],\n",
    "76:['keyboard','type','tastatura','tastatur'],\n",
    "77:['cell phone','mobile','phone','cellular telephone','cellular phone','cell','cellular','mobile phone','cellular_tele'],\n",
    "78:['microwave','zap', 'nuke','microwave oven','micro cook'],\n",
    "79:['oven','jara','kitchen','appliance','cooking'],\n",
    "80:['toaster','wassailer','bread','kitchen appliance'],\n",
    "81:['sink','go_under', 'bury','slide_down','fall_off','subside',' sump'],\n",
    "82:['refrigerator',' icebox','fridge','frigorific','refrigerator','frigor'],\n",
    "84:['book', 'reading thing','tome','cover','library',' page','reading','prirunik','literature','novel','story','book','booklet','bookman','bookworm','volume','record','script','book of account','ledger','record book','rule book','scripture','text','textbook','word','work','book of facts','fact book','playscript'],\n",
    "85:['clock','telling','wall watch','time','clock','clocking','clock time'],\n",
    "86:['vase','vza','vaseful','vaasi','vase','flower vase','vaseful'],\n",
    "87:['scissors','scissors','scissorsgrip','scissor_grip','scissor_hold','pair of scissors','scissors hold'],\n",
    "88:['teddybear'],\n",
    "89:['hairdrier'],\n",
    "90:['toothbrush','hambahari','tutbras','znnbiischt','toothbrushes','tandbrste','teeth','brush','tannkost','brosse dents','tannbrste','caday','cleaning','electric toothbrush'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77herdU_OB70"
   },
   "outputs": [],
   "source": [
    "def embed_visual_words(sentence: str, mydic: dict) -> dict:\n",
    "    visual_words = do_spacy(sentence)\n",
    "    print('visual words:',visual_words)\n",
    "    embeddings_with_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8rMvI-ZWu2j",
    "outputId": "2131478f-8f8e-41a2-e955-4e6a0ba8d8d1"
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    embeddings = embed_visual_words(sentence, mydic)\n",
    "    print(f\"Sentence: {sentence}\\nEmbeddings: {embeddings}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9YtE6goPaSM"
   },
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    # Placeholder for the heuristic.get_vector function\n",
    "    # This function should return the word embedding vector\n",
    "    return heuristic.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7hIfPEtYPEI"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def process_sentences_in_batches(file_path, mydic, batch_size=10000, allowed_labels=[1, 2, 3,4 ,5]):\n",
    "    # Read the first half of the text file\n",
    "    sentences = read_all_of_text_file(file_path)\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "        results = []\n",
    "\n",
    "        for sentence in batch_sentences:\n",
    "            print(sentence)\n",
    "            visual_words = do_spacy(sentence)\n",
    "            embeddings_with_labels = {}\n",
    "            print(\"Extracted visual words:\", visual_words)  # Debug print\n",
    "\n",
    "            for word in visual_words:\n",
    "                vector = get_vector(word)\n",
    "                label = next((key for key, value in mydic.items() if word.lower() in [v.lower() for v in value]), None)\n",
    "                print(f\"Word: '{word}', Label found: {label}\")  # Debug print\n",
    "                if label is not None and label in allowed_labels:\n",
    "                    print(f'----------The {word} in allowdLabels---------')\n",
    "                    embeddings_with_labels[tuple(vector)] = label\n",
    "\n",
    "            if embeddings_with_labels:\n",
    "                results.append(embeddings_with_labels)\n",
    "\n",
    "        # Save the filtered embeddings in a pickle file\n",
    "        pkl_filename = f'embeddings_batch_{i//batch_size + 1}.pkl'\n",
    "        with open(pkl_filename, 'wb') as pkl_file:\n",
    "            pickle.dump(results, pkl_file)\n",
    "        print(f\"Saved {pkl_filename} containing batch {i//batch_size + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HOYCIIKPmEJ",
    "outputId": "2722f4e7-9048-422f-c099-a8aff3a3ab7b"
   },
   "outputs": [],
   "source": [
    "file_path = '/content/drive/MyDrive/captions_output.txt'\n",
    "process_sentences_in_batches(file_path, mydic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtaePlZ3UUi8"
   },
   "outputs": [],
   "source": [
    "# save it as dic\n",
    "\n",
    "import pickle\n",
    "\n",
    "def process_sentences(file_path, mydic, allowed_labels=[1, 2, 3, 4, 5]):\n",
    "    # Read all sentences from the text file\n",
    "    sentences = read_all_of_text_file(file_path)\n",
    "    file_counter = 1  # Counter to name files uniquely\n",
    "\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "        visual_words = do_spacy(sentence)\n",
    "        embeddings_with_labels = {}\n",
    "        print(\"Extracted visual words:\", visual_words)  # Debug print\n",
    "\n",
    "        for word in visual_words:\n",
    "            vector = get_vector(word)\n",
    "            label = next((key for key, value in mydic.items() if word.lower() in [v.lower() for v in value]), None)\n",
    "            print(f\"Word: '{word}', Label found: {label}\")  # Debug print\n",
    "            if label is not None and label in allowed_labels:\n",
    "                print(f'----------The {word} in allowdLabels---------')\n",
    "                embeddings_with_labels[tuple(vector)] = label\n",
    "\n",
    "        if embeddings_with_labels:\n",
    "            # Save the embeddings in a pickle file\n",
    "            pkl_filename = f'embeddings_{file_counter}.pkl'\n",
    "            with open(pkl_filename, 'wb') as pkl_file:\n",
    "                pickle.dump(embeddings_with_labels, pkl_file)\n",
    "            print(f\"Saved {pkl_filename}\")\n",
    "            file_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_zQOdgbPn9X",
    "outputId": "a127a005-ad78-4b13-a361-0bc056b69568"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Step 2: Open the .pkl file in binary read mode\n",
    "with open('/content/embeddings_batch_1.pkl', 'rb') as file:\n",
    "    # Step 3: Load the data from the pickle file\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "# Step 4: Print or process the loaded data\n",
    "print(loaded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0QYqSnASTVx"
   },
   "outputs": [],
   "source": [
    "!cp '/content/embeddings_batch_10.pkl' '/content/drive/MyDrive/src_for_RunAndTest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjiGCNqHWURR"
   },
   "source": [
    "## Combine pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gchs6Kt3WR0O"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Initialize an empty list to store combined data\n",
    "combined_data = []\n",
    "\n",
    "# Loop through each file and append its contents to combined_data\n",
    "for i in range(1, 11):  # Assuming file indices from 1 to 10\n",
    "    filename = f'/content/drive/MyDrive/src_for_RunAndTest/embeddings_batch_{i}.pkl'\n",
    "\n",
    "    with open(filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        combined_data.extend(data)  # Use extend for lists\n",
    "\n",
    "# Save the combined data back to a new .pkl file\n",
    "with open('combined_embeddings.pkl', 'wb') as file:\n",
    "    pickle.dump(combined_data, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuZF6pLXlVoC"
   },
   "source": [
    "## For evalute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEWfzjfdWiUw"
   },
   "outputs": [],
   "source": [
    "def process_sentences(sentence=' ', allowed_labels=[1, 2, 3, 4, 5]):\n",
    "    # Process the sentence with spacy or a similar tool to get visual words\n",
    "    visual_words = do_spacy(sentence)\n",
    "    embeddings_with_labels = {}\n",
    "    print(\"Extracted visual words:\", visual_words)  # Debug print\n",
    "\n",
    "    for word in visual_words:\n",
    "        vector = get_vector(word)\n",
    "        label = next((key for key, value in mydic.items() if word.lower() in [v.lower() for v in value]), None)\n",
    "        print(f\"Word: '{word}', Label found: {label}\")  # Debug print\n",
    "        if label is not None and label in allowed_labels:\n",
    "            print(f'----------The {word} in allowedLabels---------')\n",
    "            # Store the word along with its vector and label\n",
    "            embeddings_with_labels[word] = {'vector': vector, 'label': label}\n",
    "\n",
    "    return embeddings_with_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4Np9By1oEmO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_sentences(sentence=' ', allowed_labels=None):\n",
    "    print(\"Processing sentence:\", sentence)\n",
    "    visual_words = do_spacy(sentence)\n",
    "    print(\"Extracted visual words:\", visual_words)\n",
    "\n",
    "    data_for_csv = []\n",
    "\n",
    "    for word in visual_words:\n",
    "        vector = get_vector(word)\n",
    "        label = next((key for key, value in mydic.items() if word.lower() in [v.lower() for v in value]), None)\n",
    "        if label is not None:\n",
    "            print(f\"Word: '{word}', Label found: {label}\")\n",
    "            if allowed_labels is None or label in allowed_labels:\n",
    "                print(f'Adding {word} with label {label} to CSV data')\n",
    "                data_for_csv.append(list(vector) + [label])\n",
    "        else:\n",
    "            print(f\"Label not found for '{word}'\")\n",
    "\n",
    "    # Create a DataFrame without column headers\n",
    "    df = pd.DataFrame(data_for_csv)\n",
    "    csv_file_name = 'output.csv'\n",
    "    df.to_csv(csv_file_name, index=False, header=False)\n",
    "    print(f\"Data saved to {csv_file_name}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M8cIoLw7sf3"
   },
   "source": [
    "### Last function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g4vcQp-X57an",
    "outputId": "50d1df3f-67f8-4f14-97b7-f226ec21ae5b"
   },
   "outputs": [],
   "source": [
    "def process_file(file_path, allowed_labels=None):\n",
    "    data_for_csv = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for sentence in file:\n",
    "            sentence = sentence.strip()  # Remove leading/trailing whitespaces\n",
    "            if sentence:  # Check if the sentence is not empty\n",
    "                print(\"Processing sentence:\", sentence)\n",
    "                visual_words = do_spacy(sentence)\n",
    "                print(\"Extracted visual words:\", visual_words)\n",
    "\n",
    "                for word in visual_words:\n",
    "                    vector = get_vector(word)\n",
    "                    label = next((key for key, value in mydic.items() if word.lower() in [v.lower() for v in value]), None)\n",
    "                    if label is not None:\n",
    "                        print(f\"Word: '{word}', Label found: {label}\")\n",
    "                        if allowed_labels is None or label in allowed_labels:\n",
    "                            print(f'Adding {word} with label {label} to CSV data')\n",
    "                            data_for_csv.append(list(vector) + [label])\n",
    "                    else:\n",
    "                        print(f\"Label not found for '{word}'\")\n",
    "\n",
    "    # Create a DataFrame without column headers\n",
    "    df = pd.DataFrame(data_for_csv)\n",
    "    csv_file_name = 'output.csv'\n",
    "    df.to_csv(csv_file_name, index=False, header=False)\n",
    "    print(f\"Data saved to {csv_file_name}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "process_file('/content/capt.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7meu5upJdelu"
   },
   "source": [
    "## Get caption and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSCRtMid5KKv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_file(file_path, allowed_labels=[1,2,3,4,5]):\n",
    "    data_for_csv = []\n",
    "    current_image_name = ''  # Initialize the current image name\n",
    "    all_captions = []  # Initialize a list to keep track of all captions\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for sentence in file:\n",
    "            sentence = sentence.strip()  # Remove leading/trailing whitespaces\n",
    "            if sentence:  # Check if the sentence is not empty\n",
    "                # Check if the line is an image name using a simple pattern match\n",
    "                if sentence.endswith('.jpg'):\n",
    "                    current_image_name = sentence  # Update the current image name\n",
    "                    continue  # Skip further processing for this line\n",
    "\n",
    "                print(\"Processing sentence:\", sentence)\n",
    "                visual_words = do_spacy(sentence)\n",
    "                print(\"Extracted visual words:\", visual_words)\n",
    "\n",
    "                for word in visual_words:\n",
    "                    vector = get_vector(word)\n",
    "                    label = next((key for key, value in mydic.items() if word.lower() in [v.lower() for v in value]), None)\n",
    "                    if label is not None:\n",
    "                        print(f\"Word: '{word}', Label found: {label}\")\n",
    "                        if allowed_labels is None or label in allowed_labels:\n",
    "                            print(f'Adding {word} with label {label} to CSV data')\n",
    "                            # Add the current image name and the original caption to each row\n",
    "                            data_for_csv.append(list(vector) + [label, current_image_name, sentence])\n",
    "                    else:\n",
    "                        print(f\"Label not found for '{word}'\")\n",
    "                all_captions.append(sentence)  # Add the sentence to all_captions list\n",
    "\n",
    "    # Create a DataFrame with specified column headers\n",
    "    # Assuming the vector has a fixed length, adjust column names accordingly\n",
    "    num_vector_elements = len(data_for_csv[0]) - 3  # Subtract label, image name, and caption columns\n",
    "    vector_column_names = [f'Feature_{i+1}' for i in range(num_vector_elements)]\n",
    "    df = pd.DataFrame(data_for_csv, columns=vector_column_names + ['Label', 'ImageName', 'Caption'])\n",
    "\n",
    "    csv_file_name = 'output.csv'\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    print(f\"Data saved to {csv_file_name}\")\n",
    "\n",
    "    return df, all_captions\n",
    "\n",
    "# Note: You will need to implement or adjust the `do_spacy`, `get_vector`, and `mydic` parts according to your specific requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_Y8ZidE5deX",
    "outputId": "cec42bbd-0a13-494a-bf24-f873b2850b64"
   },
   "outputs": [],
   "source": [
    "process_file('/content/image_captions_output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Hfe3Bkvly-N",
    "outputId": "9afd5ce3-5931-4b4e-cdbc-989882974505"
   },
   "outputs": [],
   "source": [
    "# Test the function\n",
    "test_sentence = \"I have a happy dog and a blue car\"\n",
    "result = process_sentences(test_sentence)\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gil1TR2i797s"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "zBU1lxnmoG2h",
    "outputId": "b7abd5b5-d018-46e1-f350-beac2986367b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data from CSV\n",
    "csv_file_name = 'output.csv'\n",
    "data = pd.read_csv(csv_file_name, header=None)\n",
    "\n",
    "# Separate features and labels\n",
    "features = data.iloc[:, :-1]  # All columns except the last one\n",
    "labels = data.iloc[:, -1]    # Only the last column\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA to reduce dimensions to 10\n",
    "pca = PCA(n_components=10)\n",
    "reduced_features = pca.fit_transform(features_std)\n",
    "\n",
    "# Recombine reduced features with labels\n",
    "reduced_data = pd.DataFrame(reduced_features)\n",
    "reduced_data['label'] = labels\n",
    "\n",
    "# Save the reduced data to a new CSV file\n",
    "reduced_csv_file_name = 'reduced_output_New.csv'\n",
    "reduced_data.to_csv(reduced_csv_file_name, index=False, header=False)\n",
    "print(f\"Reduced data saved to {reduced_csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z59TohbgdnnP"
   },
   "source": [
    "## PCA to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXVhd08eqVhW",
    "outputId": "65fb59c0-cd4e-402f-fbd1-9b1a568bf3ee"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Load the data from CSV\n",
    "csv_file_name = 'output.csv'\n",
    "data = pd.read_csv(csv_file_name)\n",
    "\n",
    "# Assuming the first column is not a feature but an identifier or index,\n",
    "# and the actual features are from columns 1 to 300 (Python uses 0-based indexing)\n",
    "# Separate features (columns 1 to 300) and labels (last two columns)\n",
    "features = data.iloc[:, 1:301]  # Select columns for features\n",
    "labels = data.iloc[:, -2:]      # Select the last two columns for labels (ImageName, Caption)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA to reduce dimensions to 10\n",
    "pca = PCA(n_components=10)\n",
    "reduced_features = pca.fit_transform(features_std)\n",
    "\n",
    "# Recombine reduced features with labels\n",
    "reduced_data = pd.DataFrame(reduced_features)\n",
    "# Concatenate reduced features with labels horizontally\n",
    "reduced_data = pd.concat([reduced_data, labels.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save the reduced data to a new CSV file\n",
    "reduced_csv_file_name = 'reduced_output_New.csv'\n",
    "reduced_data.to_csv(reduced_csv_file_name, index=False)\n",
    "print(f\"Reduced data saved to {reduced_csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_m7FEoaE6Hed"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ySqCfIhzLyAf",
    "ChZ2laHTNQ2z",
    "gjiGCNqHWURR"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
